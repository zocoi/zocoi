---
layout: post
title: Deploying machine learning models Docker Swarm on AWS
date: 
type: post
parent_id: '0'
published: false
password: ''
status: draft
categories:
- Misc
tags: []
meta:
  _publicize_failed_6915753: O:13:"Keyring_Error":2:{s:6:"errors";a:1:{s:21:"keyring-request-error";a:1:{i:0;a:6:{s:7:"headers";O:42:"Requests_Utility_CaseInsensitiveDictionary":1:{s:7:"
  publicize_twitter_user: zocoi
  _publicize_done_6915749: '1'
  _wpas_done_6901007: '1'
  _wpas_done_1907398: '1'
  _publicize_done_2317232: '1'
  _publicize_done_external: a:2:{s:8:"facebook";a:1:{i:1907398;s:38:"https://facebook.com/10155939914624924";}s:7:"twitter";a:1:{i:6901007;s:51:"https://twitter.com/zocoi/status/921540941458149381";}}
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '10554018210'
author:
  login: mephis1987
  email: mephis1987@gmail.com
  display_name: Hung Dao
  first_name: ''
  last_name: ''
---
<p>At Hoodline, we've been building multiple machine learning models and deploying them in both Heroku and Amazon EC2. Each model is a micro web service with standard endpoints documented by Swagger. In early stage, each model/service is owned by the machine learning engineers including development, deploying and monitoring. As our team grows, more models were built, tested and deployed. Not all models need to be in production, but once one is ready for prime time, we need to lift the burden of deployment and devops from ML engineer and make it simple to deploy.</p>
<p>First we need Docker Swarm setup on AWS, see my previous blog post for instructions. In this example we have a swarm with 3 managers and 5 workers</p>
<p><img class="alignnone size-full wp-image-1259" src="{{ site.baseurl }}/assets/download-1.png" alt="download (1).png" width="2880" height="1462" /></p>
<p><img class="alignnone size-full wp-image-1261" src="{{ site.baseurl }}/assets/download-2.png" alt="download (2).png" width="2880" height="1462" /></p>
<p>My coworker Glen created a convenient boilerplate repo which automatically includes dependencies, creates Flask/Swager web server and generates a Docker image. `Dockerfile` and `docker-compose.yml` can be extended for further customization</p>
<p>Each model has its own Github repo extending from boilerplate repo and every commit on `master` branch will trigger CircleCI to build and push a new image to our private Docker Cloud, e.g `hoodline/hl-ml-sentiment:latest`. Commit on production will push an image with version tag `hoodline/hl-ml-sentiment:1.0`</p>
<p><img class="alignnone size-full wp-image-1227" src="{{ site.baseurl }}/assets/untitled.png" alt="Untitled.png" width="2880" height="555" /></p>
<p>Part of deployment workflow, we tell CircleCI to connect to our swarm and deploy the freshly built image</p>
<p><code>docker run --rm -ti -v /var/run/docker.sock:/var/run/docker.sock -e DOCKER_HOST dockercloud/client hoodline/hl-ml-swarm</code><code></code><code></code><br />
<code>docker stack deploy -c docker-compose.yml --with-registry-auth sentiment</code></p>
<p>Once CircleCI workflow finishes which means that the app is deployed, we can confirm with</p>
<pre><span class="s2">$ docker stack services sentiment</span>

<span class="s2">ID<span class="Apple-converted-space">                  </span>NAME<span class="Apple-converted-space">                  </span>MODE<span class="Apple-converted-space">                </span>REPLICAS<span class="Apple-converted-space">            </span>IMAGE <span class="Apple-converted-space">                            </span>PORTS</span>

<span class="s2">byekwswqvcnd<span class="Apple-converted-space">        </span>sentiment_hl-ml-api <span class="Apple-converted-space">  </span>replicated<span class="Apple-converted-space">          </span>1/1 <span class="Apple-converted-space">                </span>hoodline/hl-ml-sentiment:latest <span class="Apple-converted-space">  </span>*:9090-&gt;9090/tcp</span></pre>
<p>and test the endpoints using the load balancer host</p>
<pre class="p1"><span class="s2">$ curl -v http://hl-ml-swa-External-EHMAMRIV26FT-981729072.us-west-1.elb.amazonaws.com:9090/models</span>

<span class="s2">&gt; GET /models HTTP/1.1</span>
<span class="s2">&gt; Host: hl-ml-swa-External-EHMAMRIV26FT-981729072.us-west-1.elb.amazonaws.com:9090</span>
<span class="s2">&gt; User-Agent: curl/7.54.0</span>
<span class="s2">&gt; Accept: */*</span>
<span class="s2">&gt; </span>

<span class="s2">[</span>
<span class="s2"><span class="Apple-converted-space">  </span>{</span>
<span class="s2"><span class="Apple-converted-space">    </span>"description": "Returns a `positive`, `neutral` or `negative` sentiment value for a given article title. \n<span class="Apple-converted-space">            </span>The maximum word count used is 30.",</span>
<span class="s2"><span class="Apple-converted-space">    </span>"name": "SentimentModel",</span>
<span class="s2"><span class="Apple-converted-space">    </span>"version": "1.0"</span>
<span class="s2"><span class="Apple-converted-space">  </span>}</span>
<span class="s2">]</span>
<span class="s2">* Closing connection 0</span></pre>
<p>Next time when the model is updated, the latest image will be built and pushed live. ML engineers can focus on building and trying the models.</p>
<h3>Overview</h3>
<p>Fault tolerance:</p>
<ul>
<li>Swarm can move images around once a worker node is down. Another node is delegated to run the image to cover for the missing node. Minimal downtime is guaranteed</li>
<li>The load balancer URL won't change as the swarm is up and running. No need to have different URL for each model (we can mount the LB to a DNS like: hl-ml.hoodline.com)</li>
</ul>
<p>Autoscale</p>
<ul>
<li>It is configurable to have n containers in a node and add automatically add new node as more images are being run</li>
</ul>
<p>Port partition</p>
<ul>
<li>Each model is exposed from a port and each port is unique. It is required to configure non overlapping port for each model</li>
</ul>
<p>Secrets</p>
<ul>
<li>Similar to Heroku, secrets can be sent as `​ENV` and will be populated to every node</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
